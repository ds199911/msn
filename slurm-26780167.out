INFO:root:called-params configs/pretrain/msn_fusion_random.yaml
INFO:root:loaded params...
{   'criterion': {   'batch_size': 64,
                     'ent_weight': 0.0,
                     'final_sharpen': 0.25,
                     'me_max': True,
                     'memax_weight': 1.0,
                     'num_proto': 512,
                     'start_sharpen': 0.25,
                     'temperature': 0.1,
                     'use_ent': True,
                     'use_sinkhorn': True},
    'data': {   'color_jitter_strength': 0.5,
                'focal_size': 96,
                'focal_views': 10,
                'image_folder': '/imagenet/',
                'label_smoothing': 0.0,
                'num_workers': 10,
                'patch_drop': 0.15,
                'pin_mem': True,
                'rand_size': 224,
                'rand_views': 1,
                'root_path': '.'},
    'logging': {   'folder': 'checkpoint/msn_os_logs/',
                   'write_tag': 'msn-experiment-fusion-128'},
    'meta': {   'bottleneck': 1,
                'copy_data': False,
                'drop_path_rate': 0.0,
                'hidden_dim': 2048,
                'load_checkpoint': False,
                'model_name': 'deit_small',
                'output_dim': 256,
                'read_checkpoint': None,
                'use_bn': True,
                'use_fp16': False,
                'use_pred_head': False},
    'optimization': {   'clip_grad': 3.0,
                        'epochs': 100,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 15,
                        'weight_decay': 0.04}}
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for 1 nodes.
INFO:root:Running... (rank: 0/1)
INFO:root:Running fusion
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:Fusion(
  (ehr_model): LSTM(
    (layer0): LSTM(76, 128, batch_first=True)
    (dense_layer): Linear(in_features=128, out_features=128, bias=True)
  )
  (cxr_model): VisionTransformer(
    (patch_embed): PatchEmbed(
      (proj): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))
    )
    (pos_drop): Dropout(p=0.0, inplace=False)
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=384, out_features=1152, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=384, out_features=384, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
        (mlp): MLP(
          (fc1): Linear(in_features=384, out_features=1536, bias=True)
          (act): GELU()
          (fc2): Linear(in_features=1536, out_features=384, bias=True)
          (drop): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)
    (fc): Identity()
  )
  (projection): Linear(in_features=512, out_features=512, bias=True)
  (fc): Sequential(
    (fc1): Linear(in_features=512, out_features=2048, bias=True)
    (bn1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu1): GELU()
    (fc2): Linear(in_features=2048, out_features=2048, bias=True)
    (bn2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu2): GELU()
    (fc3): Linear(in_features=2048, out_features=256, bias=True)
  )
)
INFO:root:making data transforms
INFO:root:MIMICCXR cxr_ehr fusion dataset
INFO:root:Namespace(align=0.0, batch_size=64, beta_1=0.9, crop=224, cxr_data_dir='/data/MedFuse/2.0.0', daft_activation='linear', data_pairs='partial_ehr_cxr', data_ratio=1.0, depth=1, devices=['cuda:0'], dim=256, dropout=0.0, ehr_data_dir='/data/MedFuse/mimic-iv-extracted', epochs=100, eval=False, fname='configs/pretrain/msn_fusion_random.yaml', fusion='joint', fusion_type='lstm', imputation='previous', labels_set='pheno', layer_after=4, layers=1, load_state=None, load_state_cxr=None, load_state_ehr=None, lr=0.0001, missing_token=None, mmtm_ratio=4, modality='fusion', mode='train', network=None, normalizer_state='/scratch/projects/shamoutlab/ds5749/msn/src/medfuse/normalizers/ph_ts1.0.input_str:previous.start_time:zero.normalizer', num_classes=25, patience=15, pretrained=False, rec_dropout=0.0, resize=256, resume=False, save_dir='checkpoints', task='phenotyping', timestep=1.0, vision_backbone='densenet121', vision_num_classes=14)
INFO:root:partial_ehr_cxrlstm
INFO:root:MIMICCXR dataset created
INFO:root:unsupervised data loader created
INFO:root:iterations per epoch: 666
INFO:root:Created prototypes: torch.Size([512, 256])
INFO:root:Requires grad: True
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 6.456 (6.402 0.054 6.039) (np: 59.0, max-t: 0.051) [wd: 4.00e-02] [lr: 2.00e-04] [mem: 1.24e+04] (1523 ms; 34 ms)
INFO:root:[1,     0] grad_stats: [5.48e-01 9.86e-02] (6.01e-07, 9.65e-01)
INFO:root:[1,    10] loss: 6.333 (6.318 0.014 6.141) (np: 61.5, max-t: 0.050) [wd: 4.00e-02] [lr: 2.01e-04] [mem: 1.29e+04] (748 ms; 37 ms)
INFO:root:[1,    10] grad_stats: [2.61e-02 4.14e-03] (1.28e-06, 1.79e-01)
INFO:root:[1,    20] loss: 6.293 (6.284 0.008 6.176) (np: 61.6, max-t: 0.047) [wd: 4.00e-02] [lr: 2.02e-04] [mem: 1.30e+04] (710 ms; 36 ms)
INFO:root:[1,    20] grad_stats: [4.26e-02 2.43e-03] (1.65e-05, 1.07e-01)
INFO:root:[1,    30] loss: 6.274 (6.268 0.006 6.193) (np: 61.3, max-t: 0.043) [wd: 4.00e-02] [lr: 2.02e-04] [mem: 1.30e+04] (695 ms; 36 ms)
INFO:root:[1,    30] grad_stats: [2.71e-02 1.64e-03] (2.32e-05, 7.63e-02)
INFO:root:[1,    40] loss: 6.264 (6.259 0.005 6.201) (np: 60.4, max-t: 0.039) [wd: 4.00e-02] [lr: 2.03e-04] [mem: 1.30e+04] (689 ms; 37 ms)
INFO:root:[1,    40] grad_stats: [1.72e-02 1.79e-03] (3.34e-09, 7.63e-02)
INFO:root:[1,    50] loss: 6.257 (6.254 0.004 6.206) (np: 59.6, max-t: 0.036) [wd: 4.00e-02] [lr: 2.04e-04] [mem: 1.30e+04] (685 ms; 37 ms)
INFO:root:[1,    50] grad_stats: [2.37e-02 1.48e-03] (6.49e-18, 7.80e-02)
INFO:root:[1,    60] loss: 6.252 (6.249 0.003 6.210) (np: 58.4, max-t: 0.034) [wd: 4.00e-02] [lr: 2.05e-04] [mem: 1.30e+04] (681 ms; 36 ms)
INFO:root:[1,    60] grad_stats: [2.31e-02 1.31e-03] (8.91e-12, 5.15e-02)
INFO:root:[1,    70] loss: 6.248 (6.245 0.003 6.211) (np: 57.2, max-t: 0.031) [wd: 4.00e-02] [lr: 2.06e-04] [mem: 1.30e+04] (679 ms; 36 ms)
INFO:root:[1,    70] grad_stats: [2.63e-02 1.30e-03] (0.00e+00, 6.08e-02)
