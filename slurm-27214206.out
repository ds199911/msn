INFO:root:called-params configs/pretrain/msn_ehr_lstm.yaml
INFO:root:loaded params...
{   'criterion': {   'batch_size': 64,
                     'ent_weight': 0.0,
                     'final_sharpen': 0.25,
                     'me_max': True,
                     'memax_weight': 1.0,
                     'num_proto': 128,
                     'start_sharpen': 0.25,
                     'temperature': 0.1,
                     'use_ent': True,
                     'use_sinkhorn': True},
    'data': {   'color_jitter_strength': 0.5,
                'focal_size': 96,
                'focal_views': 10,
                'image_folder': '/imagenet/',
                'label_smoothing': 0.0,
                'modality': 'ehr',
                'num_workers': 10,
                'patch_drop': 0.15,
                'pin_mem': True,
                'rand_size': 224,
                'rand_views': 1,
                'root_path': '.'},
    'logging': {   'folder': 'checkpoint/msn_ehr_logs/',
                   'write_tag': 'msn-experiment-1'},
    'meta': {   'bottleneck': 1,
                'copy_data': False,
                'drop_path_rate': 0.0,
                'hidden_dim': 1024,
                'load_checkpoint': False,
                'model_name': 'deit_small',
                'output_dim': 128,
                'read_checkpoint': None,
                'use_bn': True,
                'use_fp16': False,
                'use_pred_head': False},
    'optimization': {   'clip_grad': 3.0,
                        'epochs': 1,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 15,
                        'weight_decay': 0.04}}
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for 1 nodes.
INFO:root:Running... (rank: 0/1)
INFO:root:Running ehr
INFO:root:Initialized (rank/world-size) 0/1
INFO:root:LSTM(
  (layer0): LSTM(76, 128, batch_first=True)
  (dense_layer): Linear(in_features=128, out_features=128, bias=True)
  (fc): Sequential(
    (fc1): Linear(in_features=128, out_features=1024, bias=True)
    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu1): GELU()
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu2): GELU()
    (fc3): Linear(in_features=1024, out_features=128, bias=True)
  )
)
INFO:root:making data transforms
INFO:root:{'criterion': {'ent_weight': 0.0, 'final_sharpen': 0.25, 'me_max': True, 'memax_weight': 1.0, 'num_proto': 128, 'start_sharpen': 0.25, 'temperature': 0.1, 'batch_size': 64, 'use_ent': True, 'use_sinkhorn': True}, 'data': {'modality': 'ehr', 'color_jitter_strength': 0.5, 'pin_mem': True, 'num_workers': 10, 'image_folder': '/imagenet/', 'label_smoothing': 0.0, 'patch_drop': 0.15, 'rand_size': 224, 'focal_size': 96, 'rand_views': 1, 'focal_views': 10, 'root_path': '.'}, 'logging': {'folder': 'checkpoint/msn_ehr_logs/', 'write_tag': 'msn-experiment-1'}, 'meta': {'bottleneck': 1, 'copy_data': False, 'drop_path_rate': 0.0, 'hidden_dim': 1024, 'load_checkpoint': False, 'model_name': 'deit_small', 'output_dim': 128, 'read_checkpoint': None, 'use_bn': True, 'use_fp16': False, 'use_pred_head': False}, 'optimization': {'clip_grad': 3.0, 'epochs': 1, 'final_lr': 1e-06, 'final_weight_decay': 0.4, 'lr': 0.001, 'start_lr': 0.0002, 'warmup': 15, 'weight_decay': 0.04}}
INFO:root:MIMICCXR ehr fusion dataset
INFO:root:Namespace(align=0.0, batch_size=64, beta_1=0.9, crop=224, cxr_data_dir='/data/MedFuse/2.0.0', daft_activation='linear', data_pairs='partial_ehr_cxr', data_ratio=1.0, depth=1, devices=['cuda:0'], dim=256, dropout=0.0, ehr_data_dir='/data/MedFuse/mimic-iv-extracted', epochs=100, eval=False, fname='configs/pretrain/msn_ehr_lstm.yaml', fusion='joint', fusion_type='lstm', imputation='previous', labels_set='pheno', layer_after=4, layers=1, load_state=None, load_state_cxr=None, load_state_ehr=None, lr=0.0001, missing_token=None, mmtm_ratio=4, modality='ehr', mode='train', network=None, normalizer_state='/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/medfuse/normalizers/ph_ts1.0.input_str:previous.start_time:zero.normalizer', num_classes=25, patience=15, pretrained=False, rec_dropout=0.0, resize=256, resume=False, save_dir='checkpoints', task='phenotyping', timestep=1.0, vision_backbone='densenet121', vision_num_classes=14)
INFO:root:partial_ehr_cxrlstm
INFO:root:MIMICCXR dataset created
INFO:root:unsupervised data loader created
INFO:root:iterations per epoch: 666
INFO:root:Created prototypes: torch.Size([128, 128])
INFO:root:Requires grad: True
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 5.302 (5.009 0.292 4.529) (np: 24.0, max-t: 0.027) [wd: 4.00e-02] [lr: 2.00e-04] [mem: 5.56e+02] (94 ms; 30 ms)
INFO:root:[1,     0] grad_stats: [0.00e+00 0.00e+00] (9.82e-01, 6.96e+00)
INFO:root:[1,    10] loss: 4.902 (4.804 0.099 4.711) (np: 26.4, max-t: 0.029) [wd: 4.02e-02] [lr: 2.01e-04] [mem: 5.56e+02] (37 ms; 15 ms)
INFO:root:[1,    10] grad_stats: [0.00e+00 0.00e+00] (1.87e-01, 1.37e+00)
INFO:root:[1,    20] loss: 4.805 (4.745 0.059 4.735) (np: 27.2, max-t: 0.029) [wd: 4.06e-02] [lr: 2.02e-04] [mem: 8.06e+02] (42 ms; 20 ms)
INFO:root:[1,    20] grad_stats: [0.00e+00 0.00e+00] (2.66e-01, 1.29e+00)
INFO:root:[1,    30] loss: 4.738 (4.695 0.044 4.722) (np: 27.4, max-t: 0.031) [wd: 4.12e-02] [lr: 2.02e-04] [mem: 9.46e+02] (43 ms; 21 ms)
INFO:root:[1,    30] grad_stats: [0.00e+00 0.00e+00] (4.06e-01, 1.46e+00)
INFO:root:[1,    40] loss: 4.696 (4.660 0.035 4.707) (np: 27.9, max-t: 0.032) [wd: 4.22e-02] [lr: 2.03e-04] [mem: 9.46e+02] (44 ms; 22 ms)
INFO:root:[1,    40] grad_stats: [0.00e+00 0.00e+00] (2.39e-01, 1.03e+00)
INFO:root:[1,    50] loss: 4.656 (4.626 0.030 4.683) (np: 28.6, max-t: 0.032) [wd: 4.33e-02] [lr: 2.04e-04] [mem: 9.46e+02] (43 ms; 21 ms)
INFO:root:[1,    50] grad_stats: [0.00e+00 0.00e+00] (2.16e-01, 8.99e-01)
INFO:root:[1,    60] loss: 4.625 (4.599 0.026 4.665) (np: 28.7, max-t: 0.033) [wd: 4.48e-02] [lr: 2.05e-04] [mem: 9.46e+02] (43 ms; 21 ms)
INFO:root:[1,    60] grad_stats: [0.00e+00 0.00e+00] (3.11e-01, 1.33e+00)
INFO:root:[1,    70] loss: 4.599 (4.575 0.024 4.645) (np: 28.9, max-t: 0.033) [wd: 4.64e-02] [lr: 2.06e-04] [mem: 9.46e+02] (43 ms; 21 ms)
INFO:root:[1,    70] grad_stats: [0.00e+00 0.00e+00] (3.52e-01, 1.73e+00)
INFO:root:[1,    80] loss: 4.572 (4.550 0.022 4.626) (np: 29.2, max-t: 0.034) [wd: 4.84e-02] [lr: 2.06e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,    80] grad_stats: [0.00e+00 0.00e+00] (3.54e-01, 1.66e+00)
INFO:root:[1,    90] loss: 4.548 (4.527 0.021 4.607) (np: 29.4, max-t: 0.034) [wd: 5.05e-02] [lr: 2.07e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,    90] grad_stats: [0.00e+00 0.00e+00] (4.07e-01, 1.87e+00)
INFO:root:[1,   100] loss: 4.525 (4.504 0.021 4.591) (np: 29.6, max-t: 0.035) [wd: 5.29e-02] [lr: 2.08e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,   100] grad_stats: [0.00e+00 0.00e+00] (8.18e-01, 2.90e+00)
INFO:root:[1,   110] loss: 4.503 (4.483 0.021 4.574) (np: 29.8, max-t: 0.036) [wd: 5.56e-02] [lr: 2.09e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,   110] grad_stats: [0.00e+00 0.00e+00] (9.37e-01, 3.51e+00)
INFO:root:[1,   120] loss: 4.482 (4.462 0.021 4.558) (np: 30.0, max-t: 0.037) [wd: 5.85e-02] [lr: 2.10e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,   120] grad_stats: [0.00e+00 0.00e+00] (8.07e-01, 3.17e+00)
INFO:root:[1,   130] loss: 4.462 (4.442 0.021 4.542) (np: 30.4, max-t: 0.037) [wd: 6.16e-02] [lr: 2.10e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,   130] grad_stats: [0.00e+00 0.00e+00] (4.26e-01, 2.17e+00)
INFO:root:[1,   140] loss: 4.439 (4.419 0.020 4.526) (np: 30.7, max-t: 0.038) [wd: 6.49e-02] [lr: 2.11e-04] [mem: 9.46e+02] (42 ms; 21 ms)
INFO:root:[1,   140] grad_stats: [0.00e+00 0.00e+00] (5.91e-01, 2.41e+00)
INFO:root:[1,   150] loss: 4.418 (4.398 0.020 4.510) (np: 31.0, max-t: 0.039) [wd: 6.85e-02] [lr: 2.12e-04] [mem: 9.46e+02] (43 ms; 21 ms)
INFO:root:[1,   150] grad_stats: [0.00e+00 0.00e+00] (6.73e-01, 3.07e+00)
INFO:root:[1,   160] loss: 4.398 (4.378 0.019 4.494) (np: 31.3, max-t: 0.039) [wd: 7.23e-02] [lr: 2.13e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   160] grad_stats: [0.00e+00 0.00e+00] (6.21e-01, 2.58e+00)
INFO:root:[1,   170] loss: 4.377 (4.358 0.019 4.478) (np: 31.7, max-t: 0.040) [wd: 7.62e-02] [lr: 2.14e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   170] grad_stats: [0.00e+00 0.00e+00] (5.69e-01, 2.43e+00)
INFO:root:[1,   180] loss: 4.357 (4.337 0.020 4.462) (np: 32.1, max-t: 0.041) [wd: 8.04e-02] [lr: 2.14e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   180] grad_stats: [0.00e+00 0.00e+00] (8.68e-01, 3.16e+00)
INFO:root:[1,   190] loss: 4.339 (4.319 0.020 4.446) (np: 32.3, max-t: 0.042) [wd: 8.48e-02] [lr: 2.15e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   190] grad_stats: [0.00e+00 0.00e+00] (7.03e-01, 2.94e+00)
INFO:root:[1,   200] loss: 4.321 (4.301 0.020 4.431) (np: 32.6, max-t: 0.042) [wd: 8.94e-02] [lr: 2.16e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   200] grad_stats: [0.00e+00 0.00e+00] (7.14e-01, 2.73e+00)
INFO:root:[1,   210] loss: 4.302 (4.282 0.020 4.416) (np: 32.9, max-t: 0.043) [wd: 9.42e-02] [lr: 2.17e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   210] grad_stats: [0.00e+00 0.00e+00] (8.55e-01, 3.32e+00)
INFO:root:[1,   220] loss: 4.284 (4.263 0.021 4.400) (np: 33.2, max-t: 0.044) [wd: 9.91e-02] [lr: 2.18e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   220] grad_stats: [0.00e+00 0.00e+00] (5.79e-01, 2.41e+00)
INFO:root:[1,   230] loss: 4.266 (4.245 0.021 4.386) (np: 33.4, max-t: 0.045) [wd: 1.04e-01] [lr: 2.18e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   230] grad_stats: [0.00e+00 0.00e+00] (7.43e-01, 2.94e+00)
INFO:root:[1,   240] loss: 4.248 (4.227 0.021 4.371) (np: 33.7, max-t: 0.046) [wd: 1.10e-01] [lr: 2.19e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   240] grad_stats: [0.00e+00 0.00e+00] (1.17e+00, 4.06e+00)
INFO:root:[1,   250] loss: 4.233 (4.211 0.022 4.356) (np: 33.8, max-t: 0.046) [wd: 1.15e-01] [lr: 2.20e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   250] grad_stats: [0.00e+00 0.00e+00] (1.13e+00, 3.97e+00)
INFO:root:[1,   260] loss: 4.216 (4.194 0.022 4.342) (np: 34.1, max-t: 0.047) [wd: 1.21e-01] [lr: 2.21e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   260] grad_stats: [0.00e+00 0.00e+00] (8.80e-01, 3.71e+00)
INFO:root:[1,   270] loss: 4.200 (4.177 0.023 4.328) (np: 34.3, max-t: 0.048) [wd: 1.26e-01] [lr: 2.22e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   270] grad_stats: [0.00e+00 0.00e+00] (1.10e+00, 4.19e+00)
INFO:root:[1,   280] loss: 4.186 (4.162 0.024 4.315) (np: 34.5, max-t: 0.049) [wd: 1.32e-01] [lr: 2.23e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   280] grad_stats: [0.00e+00 0.00e+00] (1.30e+00, 4.50e+00)
INFO:root:[1,   290] loss: 4.170 (4.146 0.024 4.302) (np: 34.7, max-t: 0.049) [wd: 1.38e-01] [lr: 2.23e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   290] grad_stats: [0.00e+00 0.00e+00] (9.73e-01, 3.40e+00)
INFO:root:[1,   300] loss: 4.156 (4.131 0.024 4.289) (np: 34.9, max-t: 0.050) [wd: 1.44e-01] [lr: 2.24e-04] [mem: 9.54e+02] (43 ms; 22 ms)
INFO:root:[1,   300] grad_stats: [0.00e+00 0.00e+00] (9.25e-01, 3.54e+00)
INFO:root:[1,   310] loss: 4.140 (4.115 0.025 4.275) (np: 35.1, max-t: 0.051) [wd: 1.50e-01] [lr: 2.25e-04] [mem: 9.54e+02] (42 ms; 21 ms)
INFO:root:[1,   310] grad_stats: [0.00e+00 0.00e+00] (8.92e-01, 3.16e+00)
INFO:root:[1,   320] loss: 4.125 (4.100 0.025 4.262) (np: 35.3, max-t: 0.052) [wd: 1.57e-01] [lr: 2.26e-04] [mem: 9.54e+02] (42 ms; 21 ms)
INFO:root:[1,   320] grad_stats: [0.00e+00 0.00e+00] (1.58e+00, 5.15e+00)
INFO:root:[1,   330] loss: 4.110 (4.085 0.025 4.249) (np: 35.4, max-t: 0.053) [wd: 1.63e-01] [lr: 2.27e-04] [mem: 9.54e+02] (42 ms; 21 ms)
INFO:root:[1,   330] grad_stats: [0.00e+00 0.00e+00] (1.34e+00, 4.56e+00)
INFO:root:[1,   340] loss: 4.097 (4.071 0.026 4.237) (np: 35.6, max-t: 0.054) [wd: 1.70e-01] [lr: 2.27e-04] [mem: 9.54e+02] (42 ms; 21 ms)
INFO:root:[1,   340] grad_stats: [0.00e+00 0.00e+00] (1.15e+00, 4.15e+00)
INFO:root:[1,   350] loss: 4.085 (4.058 0.027 4.225) (np: 35.7, max-t: 0.055) [wd: 1.76e-01] [lr: 2.28e-04] [mem: 1.23e+03] (43 ms; 22 ms)
INFO:root:[1,   350] grad_stats: [0.00e+00 0.00e+00] (1.39e+00, 4.75e+00)
INFO:root:[1,   360] loss: 4.072 (4.045 0.027 4.213) (np: 35.9, max-t: 0.056) [wd: 1.83e-01] [lr: 2.29e-04] [mem: 1.23e+03] (43 ms; 22 ms)
INFO:root:[1,   360] grad_stats: [0.00e+00 0.00e+00] (1.69e+00, 5.48e+00)
INFO:root:[1,   370] loss: 4.059 (4.031 0.028 4.201) (np: 36.0, max-t: 0.056) [wd: 1.90e-01] [lr: 2.30e-04] [mem: 1.23e+03] (43 ms; 22 ms)
INFO:root:[1,   370] grad_stats: [0.00e+00 0.00e+00] (1.47e+00, 5.43e+00)
INFO:root:[1,   380] loss: 4.048 (4.020 0.028 4.191) (np: 36.2, max-t: 0.057) [wd: 1.96e-01] [lr: 2.31e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   380] grad_stats: [0.00e+00 0.00e+00] (2.05e+00, 7.17e+00)
INFO:root:[1,   390] loss: 4.038 (4.009 0.029 4.181) (np: 36.2, max-t: 0.058) [wd: 2.03e-01] [lr: 2.31e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   390] grad_stats: [0.00e+00 0.00e+00] (1.41e+00, 4.82e+00)
INFO:root:[1,   400] loss: 4.027 (3.997 0.030 4.171) (np: 36.4, max-t: 0.059) [wd: 2.10e-01] [lr: 2.32e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   400] grad_stats: [0.00e+00 0.00e+00] (2.15e+00, 7.16e+00)
INFO:root:[1,   410] loss: 4.016 (3.985 0.031 4.161) (np: 36.6, max-t: 0.060) [wd: 2.17e-01] [lr: 2.33e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   410] grad_stats: [0.00e+00 0.00e+00] (9.49e-01, 3.55e+00)
INFO:root:[1,   420] loss: 4.006 (3.975 0.031 4.151) (np: 36.7, max-t: 0.061) [wd: 2.23e-01] [lr: 2.34e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   420] grad_stats: [0.00e+00 0.00e+00] (1.38e+00, 4.79e+00)
INFO:root:[1,   430] loss: 3.995 (3.963 0.032 4.142) (np: 36.8, max-t: 0.062) [wd: 2.30e-01] [lr: 2.35e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   430] grad_stats: [0.00e+00 0.00e+00] (1.06e+00, 3.66e+00)
INFO:root:[1,   440] loss: 3.984 (3.952 0.032 4.132) (np: 37.0, max-t: 0.063) [wd: 2.37e-01] [lr: 2.35e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   440] grad_stats: [0.00e+00 0.00e+00] (1.29e+00, 4.45e+00)
INFO:root:[1,   450] loss: 3.975 (3.942 0.033 4.123) (np: 37.0, max-t: 0.063) [wd: 2.44e-01] [lr: 2.36e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   450] grad_stats: [0.00e+00 0.00e+00] (1.96e+00, 6.66e+00)
INFO:root:[1,   460] loss: 3.965 (3.931 0.034 4.113) (np: 37.2, max-t: 0.064) [wd: 2.50e-01] [lr: 2.37e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   460] grad_stats: [0.00e+00 0.00e+00] (1.81e+00, 6.19e+00)
INFO:root:[1,   470] loss: 3.955 (3.921 0.034 4.104) (np: 37.4, max-t: 0.065) [wd: 2.57e-01] [lr: 2.38e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   470] grad_stats: [0.00e+00 0.00e+00] (2.92e+00, 1.03e+01)
INFO:root:[1,   480] loss: 3.947 (3.911 0.035 4.096) (np: 37.5, max-t: 0.066) [wd: 2.64e-01] [lr: 2.39e-04] [mem: 1.43e+03] (43 ms; 22 ms)
INFO:root:[1,   480] grad_stats: [0.00e+00 0.00e+00] (2.43e+00, 8.42e+00)
INFO:root:[1,   490] loss: 3.938 (3.902 0.036 4.087) (np: 37.6, max-t: 0.067) [wd: 2.70e-01] [lr: 2.39e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   490] grad_stats: [0.00e+00 0.00e+00] (2.58e+00, 9.10e+00)
INFO:root:[1,   500] loss: 3.928 (3.892 0.036 4.078) (np: 37.7, max-t: 0.068) [wd: 2.77e-01] [lr: 2.40e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   500] grad_stats: [0.00e+00 0.00e+00] (1.47e+00, 5.12e+00)
INFO:root:[1,   510] loss: 3.920 (3.883 0.037 4.070) (np: 37.9, max-t: 0.069) [wd: 2.83e-01] [lr: 2.41e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   510] grad_stats: [0.00e+00 0.00e+00] (1.56e+00, 5.36e+00)
INFO:root:[1,   520] loss: 3.911 (3.874 0.037 4.062) (np: 38.0, max-t: 0.069) [wd: 2.90e-01] [lr: 2.42e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   520] grad_stats: [0.00e+00 0.00e+00] (1.48e+00, 5.19e+00)
INFO:root:[1,   530] loss: 3.904 (3.866 0.038 4.055) (np: 38.1, max-t: 0.070) [wd: 2.96e-01] [lr: 2.43e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   530] grad_stats: [0.00e+00 0.00e+00] (1.69e+00, 6.11e+00)
INFO:root:[1,   540] loss: 3.895 (3.857 0.038 4.047) (np: 38.2, max-t: 0.071) [wd: 3.02e-01] [lr: 2.43e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   540] grad_stats: [0.00e+00 0.00e+00] (1.72e+00, 6.27e+00)
INFO:root:[1,   550] loss: 3.887 (3.848 0.038 4.039) (np: 38.3, max-t: 0.072) [wd: 3.08e-01] [lr: 2.44e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   550] grad_stats: [0.00e+00 0.00e+00] (1.86e+00, 6.86e+00)
INFO:root:[1,   560] loss: 3.879 (3.839 0.039 4.032) (np: 38.4, max-t: 0.073) [wd: 3.14e-01] [lr: 2.45e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   560] grad_stats: [0.00e+00 0.00e+00] (2.38e+00, 8.58e+00)
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:Glascow coma scale verbal response
INFO:root:5 O
INFO:root:[1,   570] loss: 3.872 (3.832 0.040 4.025) (np: 38.5, max-t: 0.073) [wd: 3.19e-01] [lr: 2.46e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   570] grad_stats: [0.00e+00 0.00e+00] (2.05e+00, 7.45e+00)
INFO:root:[1,   580] loss: 3.864 (3.823 0.040 4.017) (np: 38.6, max-t: 0.074) [wd: 3.25e-01] [lr: 2.47e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   580] grad_stats: [0.00e+00 0.00e+00] (3.21e+00, 1.14e+01)
INFO:root:[1,   590] loss: 3.856 (3.815 0.041 4.010) (np: 38.7, max-t: 0.075) [wd: 3.30e-01] [lr: 2.47e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   590] grad_stats: [0.00e+00 0.00e+00] (2.17e+00, 7.68e+00)
INFO:root:[1,   600] loss: 3.848 (3.807 0.042 4.003) (np: 38.8, max-t: 0.076) [wd: 3.36e-01] [lr: 2.48e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   600] grad_stats: [0.00e+00 0.00e+00] (2.46e+00, 8.81e+00)
INFO:root:[1,   610] loss: 3.841 (3.799 0.042 3.996) (np: 38.9, max-t: 0.077) [wd: 3.41e-01] [lr: 2.49e-04] [mem: 1.60e+03] (43 ms; 22 ms)
INFO:root:[1,   610] grad_stats: [0.00e+00 0.00e+00] (1.09e+00, 3.91e+00)
INFO:root:[1,   620] loss: 3.834 (3.791 0.043 3.989) (np: 39.0, max-t: 0.078) [wd: 3.46e-01] [lr: 2.50e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   620] grad_stats: [0.00e+00 0.00e+00] (1.32e+00, 4.77e+00)
INFO:root:[1,   630] loss: 3.827 (3.784 0.043 3.983) (np: 39.1, max-t: 0.079) [wd: 3.51e-01] [lr: 2.51e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   630] grad_stats: [0.00e+00 0.00e+00] (2.59e+00, 9.57e+00)
INFO:root:[1,   640] loss: 3.820 (3.776 0.044 3.976) (np: 39.2, max-t: 0.079) [wd: 3.55e-01] [lr: 2.51e-04] [mem: 1.60e+03] (44 ms; 22 ms)
INFO:root:[1,   640] grad_stats: [0.00e+00 0.00e+00] (1.16e+00, 4.11e+00)
INFO:root:[1,   650] loss: 3.814 (3.769 0.045 3.970) (np: 39.3, max-t: 0.080) [wd: 3.60e-01] [lr: 2.52e-04] [mem: 1.60e+03] (44 ms; 23 ms)
INFO:root:[1,   650] grad_stats: [0.00e+00 0.00e+00] (2.66e+00, 9.66e+00)
INFO:root:[1,   660] loss: 3.807 (3.762 0.045 3.963) (np: 39.4, max-t: 0.081) [wd: 3.64e-01] [lr: 2.53e-04] [mem: 1.60e+03] (44 ms; 23 ms)
INFO:root:[1,   660] grad_stats: [0.00e+00 0.00e+00] (3.49e+00, 1.31e+01)
INFO:root:avg. loss 3.804
/ext3/medfuse/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448272031/work/aten/src/ATen/native/cudnn/RNN.cpp:924.)
  self.num_layers, self.dropout, self.training, self.bidirectional)
