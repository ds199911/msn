INFO:root:called-params configs/pretrain/msn_ehr_lstm.yaml
INFO:root:loaded params...
{   'criterion': {   'batch_size': 64,
                     'ent_weight': 0.0,
                     'final_sharpen': 0.25,
                     'me_max': True,
                     'memax_weight': 1.0,
                     'num_proto': 128,
                     'start_sharpen': 0.25,
                     'temperature': 0.1,
                     'use_ent': True,
                     'use_sinkhorn': True},
    'data': {   'augmentation': 'drop_start',
                'label_smoothing': 0.0,
                'modality': 'ehr',
                'num_workers': 10,
                'pin_mem': True,
                'views': 11},
    'logging': {   'folder': 'checkpoint/msn_ehr_logs/',
                   'write_tag': 'msn-ehr-experiment-drop_start'},
    'meta': {   'bottleneck': 1,
                'copy_data': False,
                'drop_path_rate': 0.0,
                'hidden_dim': 1024,
                'load_checkpoint': False,
                'model_name': 'lstm',
                'output_dim': 128,
                'read_checkpoint': None,
                'use_bn': True,
                'use_fp16': False,
                'use_pred_head': False},
    'optimization': {   'clip_grad': 3.0,
                        'epochs': 100,
                        'final_lr': 1e-06,
                        'final_weight_decay': 0.4,
                        'lr': 0.001,
                        'start_lr': 0.0002,
                        'warmup': 15,
                        'weight_decay': 0.04}}
INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0
INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for 1 nodes.
INFO:root:Running... (rank: 0/1)
INFO:root:Running ehr
INFO:root:LSTM(
  (layer0): LSTM(76, 128, batch_first=True)
  (dense_layer): Linear(in_features=128, out_features=128, bias=True)
  (fc): Sequential(
    (fc1): Linear(in_features=128, out_features=1024, bias=True)
    (bn1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu1): GELU()
    (fc2): Linear(in_features=1024, out_features=1024, bias=True)
    (bn2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (gelu2): GELU()
    (fc3): Linear(in_features=1024, out_features=128, bias=True)
  )
)
INFO:root:{'criterion': {'ent_weight': 0.0, 'final_sharpen': 0.25, 'me_max': True, 'memax_weight': 1.0, 'num_proto': 128, 'start_sharpen': 0.25, 'temperature': 0.1, 'batch_size': 64, 'use_ent': True, 'use_sinkhorn': True}, 'data': {'modality': 'ehr', 'pin_mem': True, 'num_workers': 10, 'label_smoothing': 0.0, 'augmentation': 'drop_start', 'views': 11}, 'logging': {'folder': 'checkpoint/msn_ehr_logs/', 'write_tag': 'msn-ehr-experiment-drop_start'}, 'meta': {'bottleneck': 1, 'copy_data': False, 'drop_path_rate': 0.0, 'hidden_dim': 1024, 'load_checkpoint': False, 'model_name': 'lstm', 'output_dim': 128, 'read_checkpoint': None, 'use_bn': True, 'use_fp16': False, 'use_pred_head': False}, 'optimization': {'clip_grad': 3.0, 'epochs': 100, 'final_lr': 1e-06, 'final_weight_decay': 0.4, 'lr': 0.001, 'start_lr': 0.0002, 'warmup': 15, 'weight_decay': 0.04}}
INFO:root:MIMICCXR ehr fusion dataset
INFO:root:Namespace(align=0.0, batch_size=64, beta_1=0.9, crop=224, cxr_data_dir='/data/MedFuse/2.0.0', daft_activation='linear', data_pairs='partial_ehr_cxr', data_ratio=1.0, depth=1, devices=['cuda:0'], dim=256, dropout=0.0, ehr_data_dir='/data/MedFuse/mimic-iv-extracted', epochs=100, eval=False, fname='configs/pretrain/msn_ehr_lstm.yaml', fusion='joint', fusion_type='lstm', imputation='previous', labels_set='pheno', layer_after=4, layers=1, load_state=None, load_state_cxr=None, load_state_ehr=None, lr=0.0001, missing_token=None, mmtm_ratio=4, modality='ehr', mode='train', network=None, normalizer_state='/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/medfuse/normalizers/ph_ts1.0.input_str:previous.start_time:zero.normalizer', num_classes=25, patience=15, pretrained=False, rec_dropout=0.0, resize=256, resume=False, save_dir='checkpoints', task='phenotyping', timestep=1.0, vision_backbone='densenet121', vision_num_classes=14)
INFO:root:partial_ehr_cxrlstm
INFO:root:MIMICCXR dataset created
INFO:root:unsupervised data loader created
INFO:root:iterations per epoch: 666
INFO:root:Created prototypes: torch.Size([128, 128])
INFO:root:Requires grad: True
INFO:root:Using AdamW
INFO:root:Epoch 1
INFO:root:[1,     0] loss: 4.812 (4.580 0.232 4.505) (np: 53.0, max-t: 0.116) [wd: 4.00e-02] [lr: 2.00e-04] [mem: 4.17e+02] (208 ms; 10 ms)
INFO:root:[1,     0] grad_stats: [0.00e+00 0.00e+00] (7.74e-01, 5.70e+00)
INFO:root:[1,    10] loss: 4.376 (4.296 0.079 4.576) (np: 54.2, max-t: 0.122) [wd: 4.00e-02] [lr: 2.01e-04] [mem: 6.79e+02] (70 ms; 9 ms)
INFO:root:[1,    10] grad_stats: [0.00e+00 0.00e+00] (3.36e-01, 2.08e+00)
INFO:root:[1,    20] loss: 4.247 (4.195 0.052 4.524) (np: 54.0, max-t: 0.127) [wd: 4.00e-02] [lr: 2.02e-04] [mem: 6.99e+02] (61 ms; 9 ms)
INFO:root:[1,    20] grad_stats: [0.00e+00 0.00e+00] (2.85e-01, 1.89e+00)
INFO:root:[1,    30] loss: 4.154 (4.112 0.042 4.464) (np: 54.7, max-t: 0.131) [wd: 4.00e-02] [lr: 2.02e-04] [mem: 6.99e+02] (59 ms; 10 ms)
INFO:root:[1,    30] grad_stats: [0.00e+00 0.00e+00] (3.16e-01, 1.83e+00)
/ext3/medfuse/lib/python3.6/site-packages/torch/nn/modules/rnn.py:683: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448272031/work/aten/src/ATen/native/cudnn/RNN.cpp:924.)
  self.num_layers, self.dropout, self.training, self.bidirectional)
Traceback (most recent call last):
  File "main.py", line 84, in <module>
    args=(args.fname, num_gpus, args.devices, args.modality, args))
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 230, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 188, in start_processes
    while not context.join():
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 150, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 0 terminated with the following error:
Traceback (most recent call last):
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/multiprocessing/spawn.py", line 59, in _wrap
    fn(i, *args)
  File "/scratch/projects/shamoutlab/ds5749/multi-modal-msn/main.py", line 69, in process_main
    return msn(params, medfuse_args)
  File "/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/msn_train.py", line 335, in main
    for itr, data in enumerate(unsupervised_loader):
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/utils/data/dataloader.py", line 561, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/ext3/medfuse/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/medfuse/ehr_dataset.py", line 96, in __getitem__
    data = self.transforms(data)
  File "/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/medfuse/ehr_dataset.py", line 231, in __call__
    img_views.append((self.drop_start(img)))
  File "/scratch/projects/shamoutlab/ds5749/multi-modal-msn/src/medfuse/ehr_dataset.py", line 213, in drop_start
    start = int(np.random.randint(low=1, high=int(max_percent*length), size=1))
  File "mtrand.pyx", line 746, in numpy.random.mtrand.RandomState.randint
  File "_bounded_integers.pyx", line 1254, in numpy.random._bounded_integers._rand_int64
ValueError: low >= high

